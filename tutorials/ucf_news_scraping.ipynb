{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial: Get the top news stories from UCF using web scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1. Verify the website is working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify that the news page is up and running by opening it in a web browser\n",
    "\n",
    "import webbrowser\n",
    "\n",
    "ucf_news_site = \"https://www.ucf.edu/news/\"\n",
    "\n",
    "webbrowser.open(ucf_news_site)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Did the website open?\n",
    "- Can you see the headlines of the first new news stories?\n",
    "- Hover over a few of the headlines... are they URLs?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2. Retrieve the HTML of the website and inspect it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the UCF news page without opening a browser window\n",
    "\n",
    "import re\n",
    "from curl_cffi import requests\n",
    "\n",
    "news_response = requests.get(ucf_news_site, impersonate=\"chrome124\")\n",
    "news_html = news_response.text\n",
    "news_html = re.sub(r\"\\s+\", \" \", news_html)\n",
    "print(\"The UCF server responded with status code: \", news_response.status_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- What was the status code?\n",
    "- What does this mean? If you don't remember, go back to the video on web scraping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out the first 500 characters of the HTML content\n",
    "\n",
    "import textwrap\n",
    "\n",
    "print(\"Here is what the first 500 characters of the response look like:\\n\")\n",
    "print(textwrap.fill(news_html[:500], width=80))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probably pretty difficult to read, right? That's because it's HTML.\n",
    "We need to parse it, but we'll get to that in a minute.\n",
    "\n",
    "For now, let's just make sure that the information we need is in there somewhere.\n",
    "\n",
    "In the cell below, change \"title here\" to the title of the first news story \n",
    "from the UCF website. *Make sure you type it exactly as it appears on the website.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_title = \"11 Lesser-known Facts about the Mayflower and Thanksgiving\"\n",
    "\n",
    "if news_title.lower() in news_html.lower():\n",
    "    print(\"Celebrate! The news article title was found in the UCF news page!\")\n",
    "    title_index = news_html.lower().index(news_title.lower())\n",
    "    print(\n",
    "        \"\\nHere is the first instance of the news article title in the HTML \"\n",
    "        \"and the surrounding HTML:\\n\"\n",
    "    )\n",
    "    window = 250\n",
    "    wrapped_text = textwrap.fill(\n",
    "        news_html[title_index - window : title_index + len(news_title) + window],\n",
    "        width=80,\n",
    "    )\n",
    "    print(wrapped_text)\n",
    "else:\n",
    "    print(\"Uh oh... The news article title was not found in the UCF news page.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Was the title of the first news story in the HTML?\n",
    "- Try replacing the title with the other titles from the website. Are they in the HTML?\n",
    "- What about the URLs? Are they in the HTML?\n",
    "- Try replacing the title with some random text. Is that in the HTML?\n",
    "\n",
    "When you're done, make sure you change the title back to the correct one and run this cell\n",
    "one more time so that the title is correct for the next step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3. Explore the HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's parse the HTML so we can extract the information we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "news_soup = BeautifulSoup(news_html, \"html5lib\")\n",
    "print(\"Let's see the pretty version of the HTML content:\\n\")\n",
    "print(news_soup.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, much easier to read, right? However, still a bit difficult to find the information we need.\n",
    "\n",
    "Let's use the `BeautifulSoup` library to see what all we can pull from the webpage in general."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"'a' tags are used for hyperlinks in HTML.\")\n",
    "print(\"'href' is the attribute that has the URL the hyperlink points to.\")\n",
    "print(\"So, if we find all 'a' tags and for each one, we get the 'href' \"\n",
    "      \"attribute, we can get all the links on the page.\")\n",
    "print(\"\\nHere are all the links on the UCF news page:\\n\")\n",
    "for link in news_soup.find_all(\"a\"):\n",
    "    print(link.get(\"href\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool, if we wanted to index an entire website, we could do that. \n",
    "But we don't want to do that. We just want the news stories.\n",
    "\n",
    "What if we just wanted all of the images that gets displayed to the page?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"'img' tags are used for images in HTML.\")\n",
    "print(\"'src' is the attribute that has the URL of the image.\")\n",
    "print(\"So, if we find all 'img' tags and for each one, we get the 'src' \"\n",
    "      \"attribute, we can get all the images on the page.\")\n",
    "\n",
    "print(\"\\nHere are all the images on the UCF news page:\\n\")\n",
    "for image in news_soup.find_all(\"img\"):\n",
    "    print(image.get(\"src\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again cool, if we wanted to grab all of the images from an entire website, \n",
    "we could do that. But we don't want to do that. \n",
    "\n",
    "What if we just wanted all of the text that gets displayed to the page without \n",
    "all of the HTML mark-up?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"In beautifulsoup, the .text attribute gives us the text inside a tag.\")\n",
    "print(\"If we do this for the 'main' tag, we get all text displayed in\"\n",
    "      \"the main tag within of this page.\")\n",
    "\n",
    "print(\"\\nHere is all the text on the UCF news page:\\n\")\n",
    "page_text = textwrap.fill(news_soup.find(\"main\").text, width=80)\n",
    "print(page_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again cool, if we wanted to grab all of the text from an entire website, \n",
    "but, we don't care about this... We just want the news stories.\n",
    "\n",
    "...but do you see a trend in what we're doing to get all of the `XYZ` we want \n",
    "from the website?\n",
    "- We're finding the tag that contains the information we want\n",
    "  - If this tag is unique, we're using `find` to get the tag\n",
    "  - If this tag is not unique, we're using `find_all` to get a list of\n",
    "  all such tags\n",
    "- We're then extracting the information we want from the tag or its contents\n",
    "\n",
    "Let's combine this all together to get the titles of the news stories from the \n",
    "UCF website."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4. Extract the titles of the news stories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start coding, let's figure out what we need and from where. Let's\n",
    "go back to the website and inspect the HTML to see if there's any 'pattern'\n",
    "in the way the titles are stored.\n",
    "\n",
    "The next cell will open up your browser again and show you the UCF website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "webbrowser.open(ucf_news_site)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Right click on the title of the first news story and click 'Inspect'.\n",
    "- Look at the HTML that gets highlighted. \n",
    "- Right click on the title of the second news story and click 'Inspect'.\n",
    "- Look at the HTML that gets highlighted.\n",
    "- Repeat for a few more news stories.\n",
    "- Do you see a pattern in the way the titles are stored?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like the titles are stored in \"span\" tags with the class \"h3 feature-title\".\n",
    "\n",
    "'h3' is a header tag, and so it is likely to be used for both news titles and other \n",
    "headings on the page.\n",
    "\n",
    "On the other hand, 'feature-title' is a class that is likely to be unique to the news \n",
    "titles.\n",
    "\n",
    "Let's use this information to extract the titles of the news stories from the UCF website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nHere are the titles of the featured articles on the UCF news page:\\n\")\n",
    "titles = news_soup.find_all(\"span\", class_=\"feature-title\")\n",
    "for title in titles:\n",
    "    print(title.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bingo!** We have the titles of the news stories from the UCF website.\n",
    "\n",
    "But what if we want the URLs of the news stories as well?\n",
    "\n",
    "Go back to the webpage and inspect the HTML again... where are the URLs associated with these titles?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "webbrowser.open(ucf_news_site)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Uh-oh!** The URLs are not inside of the 'span' tags we were using to get \n",
    "the titles.\n",
    "\n",
    "But, if you look at the HTML, you'll see that the 'span' tags are nested inside \n",
    "of 'a' tags. Can we use this?\n",
    "\n",
    "Think back to when we got all of the 'a' tags from the webpage. Do we want \n",
    "*all* of those?\n",
    "\n",
    "No, we only want the 'a' tags that contain the news stories. This is the art\n",
    "of web scraping. We need to find the right tags to get the information we want. \n",
    "Sometimes this is easy and we can find another tag or criterion to filter the \n",
    "tags we want. Other times, it's not so easy and we have to get creative. \n",
    "Fortunately, this is one of the easier cases.\n",
    "\n",
    "Look up a level from the 'a' tags... 'article' tags! These contain the 'a' tags\n",
    "and titles, and they appear to be unique to the news stories.\n",
    "\n",
    "Let's use this information to extract the URLs of the news stories from the UCF website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nHere are the titles of all the articles on the UCF news page:\\n\")\n",
    "articles = news_soup.find_all(\"article\")\n",
    "for article in articles:\n",
    "    title = article.find(\"span\", class_=\"feature-title\")\n",
    "    if title:\n",
    "        print(title.text)\n",
    "    else:\n",
    "        print(\"No title found for this article.\")\n",
    "\n",
    "    url = article.find(\"a\")\n",
    "    if url:\n",
    "        print(\"URL: \", url.get(\"href\"))\n",
    "    else:\n",
    "        print(\"No URL found for this article.\")\n",
    "    print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Woo hoo!** We have the titles and URLs of the news stories from the UCF website."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5. Getting the articles' full text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Printing the titles and URLs is great, but it's not very useful.\n",
    "\n",
    "We can add the titles and URLs to a dataframe. This will make it \n",
    "easier to work with the data further and save it to a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "print(\"The data in a dataframe:\\n\")\n",
    "\n",
    "data = []\n",
    "articles = news_soup.find_all(\"article\")\n",
    "for article in articles:\n",
    "    title = article.find(\"span\", class_=\"feature-title\")\n",
    "    url = article.find(\"a\")\n",
    "    data.append(\n",
    "        {\n",
    "            \"title\": title.text,\n",
    "            \"url\": url.get(\"href\"),\n",
    "            \"author\": \"\",\n",
    "            \"subtitle\": \"\",\n",
    "            \"date\": \"\",\n",
    "            \"text\": \"\",\n",
    "        }\n",
    "    )\n",
    "\n",
    "article_df = pd.DataFrame(data)\n",
    "print(article_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the \"author\", \"subtitle\", \"date\", and \"text\" columns are empty. \n",
    "We need to get this information, but it's on the news stories' individual \n",
    "pages.\n",
    "\n",
    "Let's look at the full text of the first one on our list in the browser..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_1_url = article_df[\"url\"][0]\n",
    "\n",
    "# Open the web page in a browser\n",
    "webbrowser.open(article_1_url)\n",
    "\n",
    "# Load the article into a BeautifulSoup object for you to play around with\n",
    "article_1_response = requests.get(article_1_url, impersonate=\"chrome124\")\n",
    "article_1_soup = BeautifulSoup(article_1_response.text, \"html5lib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the \"inspect\" tool to find where the full text is stored in the HTML.\n",
    "\n",
    "Feel free to play around in the next cell in a similar way to how we did.\n",
    "- The parsed html is in the variable `article_1_soup`\n",
    "- play around with the `find` and `find_all` functions \n",
    "- try to find the full text of the article without any ancillary text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your playground"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "===============================================================================\n",
    "OK, here's what I see:\n",
    "\n",
    "- All of the article meta-data is in a `header` tag with the class `site-header`\n",
    "  - Within this, the title is in a unique `h1` tag\n",
    "  - Within this, the subtitle is in a `div` tag with the class `lead mb-3`, \n",
    "  but `mb-3` is not unique to the subtitle\n",
    "  - Within this, the author is in a `span` tag with the word \"By\" in it\n",
    "  - Within this, the date is in a `span` tag with the class \n",
    "  `d-block d-sm-inline`, but `d-block` is not unique to the date\n",
    "- The main text is in a `div` tag with the class `post-content`\n",
    "  - Within this, each paragraph's text is in `p` tags (if I want to work with \n",
    "  the paragraphs separately)\n",
    "\n",
    "Let's use this information to extract the detailed information of the news \n",
    "stories from the UCF website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Header information\n",
    "header = article_1_soup.find(\"header\", class_=\"site-header\")\n",
    "title = header.find(\"h1\").text.strip()\n",
    "date = header.find(\"span\", class_=\"d-sm-inline\").text.strip()\n",
    "subtitle = header.find(\"div\", class_=\"lead\").text.strip()\n",
    "span_tags = header.find_all(\"span\")\n",
    "for span in span_tags:\n",
    "    if span.text.strip().startswith(\"By\"):\n",
    "        author = span.text.strip()[3:].strip()\n",
    "\n",
    "# Fulltext information\n",
    "fulltext = article_1_soup.find(\"div\", class_=\"post-content\").text.strip()\n",
    "\n",
    "# Print the information\n",
    "print(\"Title: \", title)\n",
    "print(\"\\nSubtitle: \", textwrap.fill(subtitle, width=80))\n",
    "print(\"\\nAuthor: \", author)\n",
    "print(\"\\nDate: \", date)\n",
    "print(\"\\nFull text: \", textwrap.fill(fulltext[:500], width=80), \"...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it! We have the full text of the news story from the UCF website...\n",
    "\n",
    "...for **one** news story.\n",
    "\n",
    "We need to do this for all of the news stories. We don't want to run this \n",
    "code individually for each news story. Let's turn it into a function that\n",
    "we can apply to all of the news stories!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_article_info(article_soup):\n",
    "    # Header information\n",
    "    header = article_soup.find(\"header\", class_=\"site-header\")\n",
    "    title = header.find(\"h1\").text.strip()\n",
    "    date = header.find(\"span\", class_=\"d-sm-inline\").text.strip()\n",
    "    subtitle = header.find(\"div\", class_=\"lead\").text.strip()\n",
    "    span_tags = header.find_all(\"span\")\n",
    "    for span in span_tags:\n",
    "        if span.text.strip().startswith(\"By\"):\n",
    "            author = span.text.strip()[3:].strip()\n",
    "\n",
    "    # Fulltext information\n",
    "    fulltext = article_soup.find(\"div\", class_=\"post-content\").text.strip()\n",
    "\n",
    "    return title, subtitle, author, date, fulltext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, we have a function that takes a \"soup\" object and returns the title,\n",
    "subtitle, author, date, and text of the news story.\n",
    "\n",
    "Let's try it out on the first news story."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "pprint(get_article_info(article_1_soup))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, but we knew that was going to work... let's try this on the 2nd \n",
    "news story that we haven't looked at yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_2_url = article_df[\"url\"][1]\n",
    "article_2_response = requests.get(article_2_url, impersonate=\"chrome124\")\n",
    "article_2_soup = BeautifulSoup(article_2_response.text, \"html5lib\")\n",
    "pprint(get_article_info(article_2_soup))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Great!** We have the full text of the 2nd news story from the UCF website.\n",
    "\n",
    "We now have a function that can get the full text of the news stories from\n",
    "the UCF website.\n",
    "\n",
    "Let's pull it all together to create a dataframe with the titles, URLs,\n",
    "authors, subtitles, dates, and full text of the news stories from the UCF \n",
    "website."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6. Pull it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get the data for each article individually\n",
    "\n",
    "for index, row in article_df.iterrows():\n",
    "    article_url = row[\"url\"]\n",
    "    article_response = requests.get(article_url, impersonate=\"chrome124\")\n",
    "    article_soup = BeautifulSoup(article_response.text, \"html5lib\")\n",
    "    try:\n",
    "        title, subtitle, author, date, fulltext = get_article_info(article_soup)\n",
    "        article_df.loc[index, \"author\"] = author\n",
    "        article_df.loc[index, \"subtitle\"] = subtitle\n",
    "        article_df.loc[index, \"date\"] = date\n",
    "        article_df.loc[index, \"text\"] = fulltext\n",
    "    except AttributeError:\n",
    "        print(f\"Error processing article at index {index} with URL {article_url}\")\n",
    "        print(\n",
    "            \"This article is probably not hosted on the UCF news site. \"\n",
    "            \"And so 'scraping' it would have to be handled differently \"\n",
    "            \"than the other articles... We will skip it for now.\\n\"\n",
    "        )\n",
    "\n",
    "print(\"Articles collected!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may find that some of the articles weren't successfully collected/scraped.\n",
    "This is likely because some of the URLs for the articles link to websites\n",
    "that are not formatted in the same way as the UCF website.\n",
    "\n",
    "This is a common problem with web scraping. You need to be able to handle\n",
    "these situations. For the purposes of this tutorial, we will ignore these\n",
    "articles. But know that the 'job isn't done' until you've handled these\n",
    "one way or another in your own projects.\n",
    "\n",
    "For now, let's look at the dataframe we've collected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out the first 5 articles with their information\n",
    "print(article_df.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at some summary statistics of the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_rows = article_df.shape[0]\n",
    "n_columns = article_df.shape[1]\n",
    "print(f\"The dataframe has {n_rows} articles with {n_columns} columns of data.\")\n",
    "\n",
    "fulltext_filter = article_df[\"text\"].str.len() != 0\n",
    "articles_with_text = article_df[fulltext_filter].shape[0]\n",
    "print(\n",
    "    f\"Of these, {articles_with_text} articles have fulltext \"\n",
    "    f\"associated with them that could be used for analysis.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chances are, you aren't going to want to work with the data in this notebook.\n",
    "\n",
    "Let's save the dataframe to a CSV file so that you can work with it in\n",
    "another notebook, Excel, or any other program that can read CSV files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "file = Path.cwd() / \"ucf_news_articles.csv\"\n",
    "article_df.to_csv(file, encoding=\"utf-8-sig\", index=False, header=True)\n",
    "print(f\"Data saved to {file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you have the news stories from the UCF website in a CSV file!\n",
    "\n",
    "Try opening it - does it look like what you expected?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Done..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
