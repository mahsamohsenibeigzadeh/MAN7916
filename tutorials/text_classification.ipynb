{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial: Text Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Token Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our ultimate goal is to classify entire texts into categories. For instance, is this employee review positive or negative? Is this news article about a merger or a scandal? Did this lay-off announcement provide utilitarian or normative justifications? However, a simpler version of this is to classify individual tokens. For example, is this word positive or negative? Is this word a noun or a verb? Is this word a person or an organization?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "\n",
    "dictionaries = {\n",
    "    \"pos\": [\"joy\", \"happiness\", \"love\", \"excitement\", \"delight\", \"pleasure\", \"contentment\", \"cheerful\",\n",
    "    \"optimism\", \"euphoria\", \"bliss\", \"grateful\", \"satisfied\", \"elated\", \"thrilled\", \"ecstatic\",\n",
    "    \"enthusiasm\", \"hopeful\", \"affection\", \"proud\", \"compassion\", \"warmth\", \"amusement\", \"serene\",\n",
    "    \"exhilaration\", \"inspired\", \"confidence\", \"tranquil\", \"trusting\", \"peaceful\", \"relieved\",\n",
    "    \"uplifted\", \"encouraged\", \"radiant\", \"vivacious\", \"glad\", \"playful\", \"reassured\", \"fulfilled\",\n",
    "    \"loving\", \"charmed\", \"jubilant\", \"festive\", \"giddy\", \"carefree\", \"graceful\", \"hearted\", \"motivated\",\n",
    "    \"rejoicing\", \"affectionate\", \"beaming\"],\n",
    "    \"neg\": [\"anger\", \"sadness\", \"fear\", \"anxiety\", \"grief\", \"frustration\", \"disgust\", \"guilt\",\n",
    "    \"shame\", \"hopeless\", \"loneliness\", \"resentment\", \"irritation\", \"jealousy\", \"embarrassment\",\n",
    "    \"rage\", \"misery\", \"depression\", \"bitterness\", \"doubt\", \"distrust\", \"hurt\", \"vulnerable\",\n",
    "    \"melancholy\", \"uneasy\", \"overwhelmed\", \"insecure\", \"worried\", \"defeated\", \"nervous\",\n",
    "    \"pessimistic\", \"tense\", \"gloomy\", \"disappointed\", \"distressed\", \"mournful\", \"hateful\",\n",
    "    \"desperate\", \"exhausted\", \"despair\", \"fatigue\", \"apathy\", \"alienated\", \"troubled\",\n",
    "    \"shattered\", \"tormented\", \"withdrawn\", \"irate\", \"lonely\", \"agitated\", \"powerless\"],\n",
    "    \"test\": ['positive', 'negative', 'gorilla']\n",
    "}\n",
    "\n",
    "print(\"Dictionaries loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we discussed in previous weeks, we cannot really work with text directly. Supervised machine learning algorithms generally require numerical inputs. Therefore, we need to convert text into numbers. Let's start with a simple example - one-hot encoding.\n",
    "\n",
    "In one-hot encoding, we create a vector for each word in our vocabulary. The length of the vector is equal to the size of the vocabulary. Each word is represented by a vector that has a 1 in the position corresponding to that word and 0s elsewhere. For example, if our vocabulary is `[\"the\", \"cat\", \"sat\", \"on\", \"mat\"]`, then the word \"cat\" would be represented as `[0, 1, 0, 0, 0]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "words = dictionaries['pos'] + dictionaries['neg'] + dictionaries['test']\n",
    "sentiment_words = dictionaries['pos'] + dictionaries['neg']\n",
    "labels = [1] * len(dictionaries['pos']) + [0] * len(dictionaries['neg'])\n",
    "\n",
    "tfidf_vectorizer = CountVectorizer(vocabulary=words, binary=True)\n",
    "X_train = tfidf_vectorizer.fit_transform(sentiment_words).toarray()\n",
    "y_train = np.array(labels)\n",
    "\n",
    "print(\"Labels: 1 = positive, 0 = negative\\n\")\n",
    "print(\"The first three training words, their vectors, and their labels:\")\n",
    "print(\"-\"*50)\n",
    "for i in range(3):\n",
    "    print(f\"{sentiment_words[i]}:\\n\\tVector: {X_train[i]}\\n\\tLabel: {y_train[i]}\\n\")\n",
    "\n",
    "print(\"\\nThe last three training words, their vectors, and their labels:\")\n",
    "print(\"-\"*50)\n",
    "for i in range(3):\n",
    "    print(f\"{sentiment_words[-(i+1)]}:\\n\\tVector: {X_train[-(i+1)]}\\n\\tLabel: {y_train[-(i+1)]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK! We have a way to represent words as numbers. But how do we classify them? Well, in the previous tutorial, we learned that linear regression could work for binary classification. We can use the same approach here. We can train a linear regression model to predict the class of each word based on its one-hot encoding. Let's try it out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "classifier = LogisticRegression(random_state=24601)\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Test the classifier on a few words\n",
    "sample_words = ['exhilaration', 'tense', 'giddy', 'hateful']\n",
    "\n",
    "X_test = tfidf_vectorizer.transform(sample_words).toarray()\n",
    "predictions = classifier.predict(X_test)\n",
    "print(\"Predictions for sample words:\")\n",
    "for word, prediction in zip(sample_words, predictions):\n",
    "    sentiment = \"positive\" if prediction == 1 else \"negative\"\n",
    "    print(f\"{word}: {sentiment}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That looks right! Those positive and negative words from the training set were classified correctly. But what about words that were not in the training set? For example, our test dataset comprises \"positively\", \"negatively\", and \"gorilla\". These should map to positive, negative, and unknown classes, respectively. Let's see what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = tfidf_vectorizer.transform(dictionaries['test']).toarray()\n",
    "predictions = classifier.predict(X_test)\n",
    "print(\"Predictions for sample words:\")\n",
    "for word, prediction in zip(dictionaries['test'], predictions):\n",
    "    sentiment = \"positive\" if prediction == 1 else \"negative\"\n",
    "    print(f\"{word}: {sentiment}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That doesn't look quite right. Everything is classified as negative. Could this be because we use Linear Regression? Let's try a different approach. How about we use Naive Bayes? It performed best last time around... right?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "param_dist = {\"var_smoothing\": np.logspace(-12, -3, 100)}\n",
    "\n",
    "nb_kfoldgrid = GaussianNB()\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=nb_kfoldgrid,\n",
    "    param_distributions=param_dist,\n",
    ")\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Test the classifier on a few words\n",
    "sample_words = ['exhilaration', 'tense', 'giddy', 'hateful']\n",
    "\n",
    "X_test = tfidf_vectorizer.transform(sample_words).toarray()\n",
    "predictions = random_search.best_estimator_.predict(X_test)\n",
    "print(\"Predictions for sample words:\")\n",
    "for word, prediction in zip(sample_words, predictions):\n",
    "    sentiment = \"positive\" if prediction == 1 else \"negative\"\n",
    "    print(f\"{word}: {sentiment}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, that still looks good. What about the test set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = tfidf_vectorizer.transform(dictionaries['test']).toarray()\n",
    "predictions = random_search.best_estimator_.predict(X_test)\n",
    "print(\"Predictions for sample words:\")\n",
    "for word, prediction in zip(dictionaries['test'], predictions):\n",
    "    sentiment = \"positive\" if prediction == 1 else \"negative\"\n",
    "    print(f\"{word}: {sentiment}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um... nope. That doesn't look good either. It seems like Naive Bayes is also struggling with words that were not in the training set. Why is that?\n",
    "\n",
    "Let's look at the training set again. Remember that the word encodings are one-hot vectors. Each word is represented by a vector that has a 1 in the position corresponding to that word and 0s elsewhere. For example, if our vocabulary is `[\"the\", \"cat\", \"sat\", \"on\", \"mat\"]`, then the word \"cat\" would be represented as `[0, 1, 0, 0, 0]`. So, when the machine learning algorithm learns from the training set, it only sees the words that are in the training set. And because there is a one-to-one mapping between words and their encodings, it cannot generalize to words that are not in the training set. This is why both Linear Regression and Naive Bayes struggled with the test set. \n",
    "\n",
    "Let's try a different approach. Instead of using one-hot encoding, let's use dense, lower-dimensional word embeddings. Recall from last week, with denser word embeddings, we can represent words as vectors in a lower-dimensional space. And importantly, these vectors can capture semantic relationships between words. Two words with similar meanings are likely to have similar vectors. For example, the words \"cat\" and \"dog\" might be represented by vectors that are close together in this space. This stands in stark contrast to one-hot encoding, where the vectors for \"cat\" and \"dog\" would be orthogonal to each other and therefore knowing about one tells you nothing about the other.\n",
    "\n",
    "We'll use the GloVe embeddings from Stanford that we used last week. Let's load them up and see what we can do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_data_path = Path().resolve().parent / \"local_data\"\n",
    "assert local_data_path.exists(), \"Data path does not exist\"\n",
    "glove_file = local_data_path / f\"glove.6B.100d.txt\"\n",
    "embeddings = {}\n",
    "with open(glove_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        vector = list(map(float, values[1:]))\n",
    "        embeddings[word] = vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace the vectorizer with a custom one that uses GloVe embeddings\n",
    "\n",
    "class GloVeVectorizer:\n",
    "    def __init__(self, embeddings):\n",
    "        self.embeddings = embeddings\n",
    "\n",
    "    def transform(self, words):\n",
    "        vectors = []\n",
    "        for word in words:\n",
    "            if word in self.embeddings:\n",
    "                vectors.append(self.embeddings[word])\n",
    "            else:\n",
    "                vectors.append([0.0] * 100)\n",
    "        return np.array(vectors)\n",
    "\n",
    "glove_vectorizer = GloVeVectorizer(embeddings)\n",
    "X_train = glove_vectorizer.transform(sentiment_words)\n",
    "print(\"Labels: 1 = positive, 0 = negative\\n\")\n",
    "print(\"The first three training words, their vectors, and their labels:\")\n",
    "print(\"-\"*50)\n",
    "for i in range(3):\n",
    "    print(f\"{sentiment_words[i]}:\\n\\tVector: {X_train[i]}\\n\\tLabel: {y_train[i]}\\n\")\n",
    "\n",
    "print(\"\\nThe last three training words, their vectors, and their labels:\")\n",
    "print(\"-\"*50)\n",
    "for i in range(3):\n",
    "    print(f\"{sentiment_words[-(i+1)]}:\\n\\tVector: {X_train[-(i+1)]}\\n\\tLabel: {y_train[-(i+1)]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, that looks better: our vectors have non-zero values for all dimensions. Let's train the linear regression model again and see what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = LogisticRegression(random_state=24601)\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Test the classifier on a few words\n",
    "sample_words = ['exhilaration', 'tense', 'giddy', 'hateful']\n",
    "\n",
    "X_test = glove_vectorizer.transform(sample_words)\n",
    "predictions = classifier.predict(X_test)\n",
    "print(\"Predictions for sample words:\")\n",
    "for word, prediction in zip(sample_words, predictions):\n",
    "    sentiment = \"positive\" if prediction == 1 else \"negative\"\n",
    "    print(f\"{word}: {sentiment}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That looks right, but that was correct last time around as well. Let's try the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = glove_vectorizer.transform(dictionaries['test'])\n",
    "predictions = classifier.predict(X_test)\n",
    "print(\"Predictions for sample words:\")\n",
    "for word, prediction in zip(dictionaries['test'], predictions):\n",
    "    sentiment = \"positive\" if prediction == 1 else \"negative\"\n",
    "    print(f\"{word}: {sentiment}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hey! Closer! \"positively\" and \"negatively\" are classified correctly. But \"gorilla\" is classified as positive. What went wrong?\n",
    "\n",
    "Let's think back to how the model was trained. The model learned two things: Some words are positive, and some words are negative (along with a sense for how to discern these apart). It never learned that there are words that are neither positive nor negative. So, when it encounters a word that it has never seen before, it has no way of knowing what to do with it. It has no way of knowing that \"gorilla\" is neither positive nor negative. It just knows that it is not like the positive words and not like the negative words. So, it classifies it as negative because that happens to be the \"closer\" category.\n",
    "\n",
    "We won't worry about this for now. For now, let's move on to the next step: classifying entire texts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Text Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous step, we classified individual tokens. Now, we want to classify entire texts. Last week we used the 20-Newsgroups dataset to do some topic modeling. Now let's use that same dataset to do some text classification. \n",
    "\n",
    "A great way of doing this would be to use the same approach as last week: getting document embeddings from a large language model (e.g., OpenAI). However, for a dataset this big, that's a bit expensive. Let's try a simpler approach. We'll use TF-IDF to get document embeddings. TF-IDF is a way of representing documents as vectors in a lower-dimensional space. It works by assigning a weight to each word in the document based on its frequency in the document and its inverse frequency in the corpus. The idea is that words that are common in the corpus but rare in the document are more informative than words that are common in both the corpus and the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the 20 newsgroups dataset\n",
    "import re\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "print(\"Getting the 20 newsgroups dataset... this may take a few minutes...\")\n",
    "newsgroups = fetch_20newsgroups(subset='all', shuffle=True, random_state=24601)\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "def preprocess(text: str) -> list:\n",
    "    text = re.sub(r'[^A-Za-z]', ' ', text)\n",
    "    words = word_tokenize(text.lower())\n",
    "    words = [word.lower() for word in words if word not in stop_words and word.isalpha()]\n",
    "    return \" \".join(words)\n",
    "\n",
    "newsgroups_data = pd.DataFrame({'text': newsgroups.data, 'label': newsgroups.target})\n",
    "newsgroups_data['text'] = newsgroups_data['text'].apply(preprocess)\n",
    "X_train, X_test, y_train, y_test = train_test_split(newsgroups_data['text'], newsgroups_data['label'], test_size=0.2, random_state=24601)\n",
    "print(\"20 newsgroups dataset loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import RandomizedSearchCV, KFold\n",
    "from sklearn.metrics import make_scorer, matthews_corrcoef, classification_report\n",
    "from scipy.stats import loguniform\n",
    "\n",
    "\n",
    "print(\"Training a logistic regression model on the 20 newsgroups dataset. This will take a while...\")\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words=stop_words, max_features=5000)\n",
    "X_train_v = tfidf_vectorizer.fit_transform(X_train)\n",
    "X_test_v = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "param_dist = {\n",
    "    'C': loguniform(1e-5, 1e5),\n",
    "    'solver': ['lbfgs', 'liblinear', 'newton-cg', 'saga'],\n",
    "    'penalty': ['l2'],\n",
    "    'max_iter': [100, 500, 1000],\n",
    "    'tol': loguniform(1e-6, 1e-2),\n",
    "    'class_weight': [None, 'balanced']\n",
    "}\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=24601)\n",
    "mcc_scorer = make_scorer(matthews_corrcoef)\n",
    "\n",
    "logistic = LogisticRegression(max_iter=1000)\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=logistic,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=100,\n",
    "    scoring=mcc_scorer,\n",
    "    cv=kfold,\n",
    "    verbose=1,\n",
    "    n_jobs=-1,\n",
    "    random_state=24601\n",
    ")\n",
    "random_search.fit(X_train_v, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = random_search.best_estimator_.predict(X_test_v)\n",
    "\n",
    "# Evaluate the model\n",
    "print(f'MCC: {matthews_corrcoef(y_test, y_pred)}')\n",
    "print(classification_report(y_test, y_pred, target_names=newsgroups.target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks good! The MCC is 0.88, which is pretty good. However, we also see that not every category is classified equally well. For example, the \"talk.religion.misc\" category has a F1 score of 0.77, which is decent, but not great. In contrast, the \"talk.politics.mideast\" category has a F1 score of 0.95, which is excellent. \n",
    "\n",
    "Let's see the confusion matrix to see what the religion category is getting confused with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=newsgroups.target_names, yticklabels=newsgroups.target_names)\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like it gets confused with alt.atheism and soc.religion.christian. This makes sense: these categories are all related to religion, so it's not surprising that they get confused with each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's try using our trained classifier on some new data unseen by the classifier training process. I used ChatGPT to generate some text for us to classify. Let's see how it does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_statements = [\n",
    "    \"I don't believe in any gods, and I think science explains everything.\",\n",
    "    \"Religious texts are just ancient stories with cultural significance.\",\n",
    "    \"I need help rendering 3D models using OpenGL.\",\n",
    "    \"Photoshop filters can create some amazing visual effects.\",\n",
    "    \"Windows keeps crashing after the latest update!\",\n",
    "    \"How do I change the registry settings in Windows 11?\",\n",
    "    \"What’s the best graphics card for gaming under $500?\",\n",
    "    \"My PC won’t boot—could it be a power supply issue?\",\n",
    "    \"Is the M2 chip significantly faster than the M1 for video editing?\",\n",
    "    \"My MacBook’s battery drains too fast—any suggestions?\",\n",
    "    \"How do I configure Xorg settings for dual monitors?\",\n",
    "    \"My Linux desktop environment isn't rendering fonts correctly.\",\n",
    "    \"Selling a barely used RTX 3090, DM for details.\",\n",
    "    \"Looking for a second-hand iPhone 13, must be in good condition.\",\n",
    "    \"Should I go for a Tesla Model 3 or a Toyota Camry hybrid?\",\n",
    "    \"My car makes a weird knocking sound—what could be wrong?\",\n",
    "    \"Best beginner-friendly motorcycle for commuting?\",\n",
    "    \"My bike won’t start in cold weather—what could be the problem?\",\n",
    "    \"Who do you think will win the World Series this year?\",\n",
    "    \"Barry Bonds should definitely be in the Hall of Fame.\",\n",
    "    \"The Maple Leafs might finally break their playoff curse this season!\",\n",
    "    \"Which NHL goalie has the best save percentage this year?\",\n",
    "    \"AES encryption is still secure, but quantum computing could change that.\",\n",
    "    \"How does RSA encryption work in simple terms?\",\n",
    "    \"I need help designing a simple transistor amplifier circuit.\",\n",
    "    \"What’s the difference between AC and DC motors?\",\n",
    "    \"Is intermittent fasting really effective for weight loss?\",\n",
    "    \"What are the long-term effects of taking antibiotics frequently?\",\n",
    "    \"NASA just announced a new mission to explore Europa!\",\n",
    "    \"Could humans realistically colonize Mars within the next 50 years?\",\n",
    "    \"What does the Bible say about forgiveness?\",\n",
    "    \"I’m looking for a good church in my area—any recommendations?\",\n",
    "    \"Should stricter gun laws be implemented to reduce crime?\",\n",
    "    \"The Second Amendment guarantees the right to bear arms, but what about regulations?\",\n",
    "    \"The Israel-Palestine conflict has deep historical roots.\",\n",
    "    \"What are the latest peace efforts in the Middle East?\",\n",
    "    \"The next presidential election will be crucial for climate policies.\",\n",
    "    \"How does the electoral college impact voting outcomes in the US?\",\n",
    "    \"Buddhism teaches mindfulness and detachment from material desires.\",\n",
    "    \"What are the core beliefs of Hinduism compared to Christianity?\"\n",
    "]\n",
    "\n",
    "for text in test_statements:\n",
    "    text_processed = preprocess(text)\n",
    "    text_vectorized = tfidf_vectorizer.transform([text_processed])\n",
    "    prediction = random_search.best_estimator_.predict(text_vectorized)\n",
    "    print(f'Text: {text} -- {newsgroups.target_names[prediction[0]]}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have accomplished two things here:\n",
    "* We have classified entire texts rather than individual tokens.\n",
    "* We have seen how these classifiers can be used to classify texts into multiple categories.\n",
    "\n",
    "Done..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
