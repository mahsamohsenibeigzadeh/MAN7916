{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LU5hehtFOEzR",
        "outputId": "ef1ae655-ed3f-41c2-84ed-ed350b552f80"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: python-docx in /usr/local/lib/python3.11/dist-packages (1.1.2)\n",
            "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from python-docx) (5.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.9.0 in /usr/local/lib/python3.11/dist-packages (from python-docx) (4.12.2)\n",
            "Loading GloVe model...\n",
            "GloVe model loaded successfully!\n",
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import gensim.downloader as api\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "import csv\n",
        "!pip install python-docx\n",
        "# Check if python-docx is installed\n",
        "try:\n",
        "    import docx\n",
        "except ModuleNotFoundError:\n",
        "    print(\"Error: The 'python-docx' library is not installed.\")\n",
        "    print(\"Please install it using: pip install python-docx\")\n",
        "    exit(1)  # Exit the program if the module is not installed\n",
        "\n",
        "# Load GloVe model using gensim's built-in downloader\n",
        "print(\"Loading GloVe model...\")\n",
        "glove_model = api.load(\"glove-wiki-gigaword-100\")  # 100-dimensional GloVe embeddings\n",
        "print(\"GloVe model loaded successfully!\")\n",
        "\n",
        "# Root words\n",
        "root_words = [\"entrepreneurial\", \"creative\", \"innovative\", \"trailblazing\"]\n",
        "\n",
        "# Get vectors for root words\n",
        "root_vectors = [glove_model[word] for word in root_words]\n",
        "\n",
        "# Calculate cosine similarity between each pair of root words\n",
        "similarities = {}\n",
        "for i, word1 in enumerate(root_words):\n",
        "    for j, word2 in enumerate(root_words):\n",
        "        if i < j:\n",
        "            sim = cosine_similarity([root_vectors[i]], [root_vectors[j]])[0][0]\n",
        "            similarities[f\"{word1}-{word2}\"] = sim\n",
        "\n",
        "# Save similarities to word_embedding_results.docx\n",
        "doc = docx.Document()  # This will work because docx is imported successfully\n",
        "doc.add_paragraph(\"Cosine Similarities between Root Words:\")\n",
        "for pair, sim in similarities.items():\n",
        "    doc.add_paragraph(f\"{pair}: {sim:.4f}\")\n",
        "\n",
        "# Identify and drop the word with the lowest average similarity\n",
        "avg_similarities = {word: np.mean([sim for pair, sim in similarities.items() if word in pair]) for word in root_words}\n",
        "word_to_drop = min(avg_similarities, key=avg_similarities.get)\n",
        "remaining_words = [word for word in root_words if word != word_to_drop]\n",
        "remaining_vectors = [glove_model[word] for word in remaining_words]\n",
        "\n",
        "# Calculate the average vector of the remaining words\n",
        "avg_vector = np.mean(remaining_vectors, axis=0)\n",
        "\n",
        "# Save the first five dimensions of the average vector\n",
        "doc.add_paragraph(\"\\nFirst five dimensions of the average vector:\")\n",
        "doc.add_paragraph(str(avg_vector[:5]))\n",
        "\n",
        "# Calculate cosine similarity between the average vector and each root word\n",
        "avg_similarities = {word: cosine_similarity([glove_model[word]], [avg_vector])[0][0] for word in remaining_words}\n",
        "doc.add_paragraph(\"\\nCosine Similarities between Average Vector and Root Words:\")\n",
        "for word, sim in avg_similarities.items():\n",
        "    doc.add_paragraph(f\"{word}: {sim:.4f}\")\n",
        "\n",
        "# Find the 50 words closest to the average vector\n",
        "deductive_words = glove_model.similar_by_vector(avg_vector, topn=50)\n",
        "\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Load the article and find the 50 closest words\n",
        "with open('/content/drive/My Drive/test1/article_preprint.txt', 'r') as file:\n",
        "    article_text = file.read().split()\n",
        "\n",
        "# Filter words that are in the GloVe model\n",
        "article_words = [word for word in article_text if word in glove_model]\n",
        "\n",
        "# Calculate similarity to the average vector for each word in the article\n",
        "article_similarities = {word: cosine_similarity([glove_model[word]], [avg_vector])[0][0] for word in article_words}\n",
        "\n",
        "# Sort by similarity and take the top 50\n",
        "inductive_words = sorted(article_similarities.items(), key=lambda x: x[1], reverse=True)[:50]\n",
        "\n",
        "# Combine the two lists and remove duplicates\n",
        "combined_words = {word: sim for word, sim in deductive_words}\n",
        "combined_words.update({word: sim for word, sim in inductive_words})\n",
        "\n",
        "# Save to CSV\n",
        "with open('word_list_for_evaluation.csv', 'w', newline='') as csvfile:\n",
        "    writer = csv.writer(csvfile)\n",
        "    writer.writerow(['word', 'score', 'eval'])\n",
        "    for word, sim in combined_words.items():\n",
        "        writer.writerow([word, sim, ''])\n",
        "\n",
        "# Save the Word document\n",
        "doc.save('word_embedding_results.docx')"
      ]
    }
  ]
}