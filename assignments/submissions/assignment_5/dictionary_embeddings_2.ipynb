{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2qvdPPVATMJQ",
        "outputId": "c064d009-02ae-4b29-fdd3-2c502cc127ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting python-docx\n",
            "  Downloading python_docx-1.1.2-py3-none-any.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from python-docx) (5.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.9.0 in /usr/local/lib/python3.11/dist-packages (from python-docx) (4.12.2)\n",
            "Downloading python_docx-1.1.2-py3-none-any.whl (244 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/244.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.3/244.3 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: python-docx\n",
            "Successfully installed python-docx-1.1.2\n",
            "\n",
            "=== Step 1: Mounting Google Drive ===\n",
            "Mounted at /content/drive\n",
            "\n",
            "=== Step 2: Reading CSV file ===\n",
            "✓ CSV file loaded successfully\n",
            "Total rows: 84\n",
            "\n",
            "First few rows:\n",
            "              word     score  eval\n",
            "0         creative  0.907521   NaN\n",
            "1       innovative  0.895459   NaN\n",
            "2  entrepreneurial  0.856225   NaN\n",
            "3       innovation  0.782994   NaN\n",
            "4       creativity  0.737144   NaN\n",
            "\n",
            "Score statistics:\n",
            "count    84.000000\n",
            "mean      0.600273\n",
            "std       0.079888\n",
            "min       0.509404\n",
            "25%       0.542408\n",
            "50%       0.587261\n",
            "75%       0.623841\n",
            "max       0.907521\n",
            "Name: score, dtype: float64\n",
            "\n",
            "=== Step 3: Filtering words based on score ===\n",
            "Words with score >= 0.8: 3\n",
            "Selected words and scores:\n",
            "  creative: 0.908\n",
            "  innovative: 0.895\n",
            "  entrepreneurial: 0.856\n",
            "\n",
            "=== Step 4: Loading GloVe model ===\n",
            "[==================================================] 100.0% 128.1/128.1MB downloaded\n",
            "✓ GloVe model loaded successfully\n",
            "\n",
            "=== Step 5: Converting words to vectors ===\n",
            "✓ Vector created for: creative (score: 0.908)\n",
            "✓ Vector created for: innovative (score: 0.895)\n",
            "✓ Vector created for: entrepreneurial (score: 0.856)\n",
            "\n",
            "=== Step 6: Creating visualization ===\n",
            "✓ Plot saved successfully at: /content/drive/My Drive/test1/dictionary_embedding_plot.jpg\n",
            "  File size: 181.27 KB\n",
            "\n",
            "=== Step 7: Creating Word document ===\n",
            "✓ Word document saved successfully at: /content/drive/My Drive/test1/word_embedding_results.docx\n",
            "  File size: 101.45 KB\n",
            "\n",
            "=== Analysis completed successfully! ===\n",
            "Files saved in: /content/drive/My Drive/test1/\n",
            "1. Plot: dictionary_embedding_plot.jpg\n",
            "2. Report: word_embedding_results.docx\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.manifold import TSNE\n",
        "import matplotlib.pyplot as plt\n",
        "import gensim.downloader as api\n",
        "!pip install python-docx\n",
        "# Check if python-docx is installed\n",
        "try:\n",
        "    import docx\n",
        "except ModuleNotFoundError:\n",
        "    print(\"Error: The 'python-docx' library is not installed.\")\n",
        "    print(\"Please install it using: pip install python-docx\")\n",
        "    exit(1)  # Exit the program if the module is not installed\n",
        "from docx import Document\n",
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "def verify_file_exists(file_path, file_type):\n",
        "    \"\"\"Check if file exists and print its size\"\"\"\n",
        "    if os.path.exists(file_path):\n",
        "        size = os.path.getsize(file_path)\n",
        "        print(f\"✓ {file_type} saved successfully at: {file_path}\")\n",
        "        print(f\"  File size: {size/1024:.2f} KB\")\n",
        "        return True\n",
        "    else:\n",
        "        print(f\"✗ Error: {file_type} not found at: {file_path}\")\n",
        "        return False\n",
        "\n",
        "def create_directory_if_not_exists(path):\n",
        "    \"\"\"Create directory if it doesn't exist\"\"\"\n",
        "    directory = os.path.dirname(path)\n",
        "    if not os.path.exists(directory):\n",
        "        os.makedirs(directory)\n",
        "        print(f\"Created directory: {directory}\")\n",
        "\n",
        "def main():\n",
        "    # 1. اتصال به Google Drive\n",
        "    print(\"\\n=== Step 1: Mounting Google Drive ===\")\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "    base_path = '/content/drive/My Drive/test1/'\n",
        "    create_directory_if_not_exists(base_path)\n",
        "\n",
        "    # 2. خواندن فایل CSV\n",
        "    print(\"\\n=== Step 2: Reading CSV file ===\")\n",
        "    csv_path = base_path + 'word_list_for_evaluation.csv'\n",
        "    try:\n",
        "        df = pd.read_csv(csv_path)\n",
        "        print(f\"✓ CSV file loaded successfully\")\n",
        "        print(f\"Total rows: {len(df)}\")\n",
        "        print(\"\\nFirst few rows:\")\n",
        "        print(df.head())\n",
        "        print(\"\\nScore statistics:\")\n",
        "        print(df['score'].describe())\n",
        "    except Exception as e:\n",
        "        print(f\"✗ Error reading CSV: {str(e)}\")\n",
        "        return\n",
        "\n",
        "    # 3. فیلتر کردن کلمات بر اساس score به جای eval\n",
        "    print(\"\\n=== Step 3: Filtering words based on score ===\")\n",
        "    # انتخاب کلمات با score بالاتر از 0.8\n",
        "    threshold = 0.8\n",
        "    df_filtered = df[df['score'] >= threshold].sort_values('score', ascending=False)\n",
        "    included_words = df_filtered['word'].tolist()\n",
        "\n",
        "    print(f\"Words with score >= {threshold}: {len(included_words)}\")\n",
        "    if len(included_words) < 2:\n",
        "        print(\"✗ Error: Need at least 2 words with high scores\")\n",
        "        # اگر کلمات کافی نبود، آستانه را کاهش می‌دهیم\n",
        "        threshold = 0.7\n",
        "        df_filtered = df[df['score'] >= threshold].sort_values('score', ascending=False)\n",
        "        included_words = df_filtered['word'].tolist()\n",
        "        print(f\"Trying with lower threshold {threshold}: {len(included_words)} words\")\n",
        "\n",
        "    if len(included_words) < 2:\n",
        "        print(\"✗ Error: Still not enough words\")\n",
        "        return\n",
        "\n",
        "    print(\"Selected words and scores:\")\n",
        "    for word, score in zip(df_filtered['word'], df_filtered['score']):\n",
        "        print(f\"  {word}: {score:.3f}\")\n",
        "\n",
        "    # 4. بارگذاری مدل GloVe\n",
        "    print(\"\\n=== Step 4: Loading GloVe model ===\")\n",
        "    try:\n",
        "        glove_model = api.load(\"glove-wiki-gigaword-100\")\n",
        "        print(\"✓ GloVe model loaded successfully\")\n",
        "    except Exception as e:\n",
        "        print(f\"✗ Error loading GloVe model: {str(e)}\")\n",
        "        return\n",
        "\n",
        "    # 5. تبدیل کلمات به بردار\n",
        "    print(\"\\n=== Step 5: Converting words to vectors ===\")\n",
        "    word_vectors = []\n",
        "    valid_words = []\n",
        "    valid_scores = []\n",
        "    for word, score in zip(df_filtered['word'], df_filtered['score']):\n",
        "        try:\n",
        "            vector = glove_model[word]\n",
        "            word_vectors.append(vector)\n",
        "            valid_words.append(word)\n",
        "            valid_scores.append(score)\n",
        "            print(f\"✓ Vector created for: {word} (score: {score:.3f})\")\n",
        "        except KeyError:\n",
        "            print(f\"✗ Warning: '{word}' not in vocabulary\")\n",
        "\n",
        "    if len(valid_words) < 2:\n",
        "        print(\"✗ Error: Not enough valid words\")\n",
        "        return\n",
        "\n",
        "    # 6. اجرای t-SNE و ایجاد نمودار\n",
        "    print(\"\\n=== Step 6: Creating visualization ===\")\n",
        "    try:\n",
        "        word_vectors_array = np.array(word_vectors)\n",
        "        perplexity_value = max(2, min(5, len(valid_words) - 1))\n",
        "        tsne = TSNE(n_components=2, perplexity=perplexity_value, random_state=42)\n",
        "        word_vectors_2d = tsne.fit_transform(word_vectors_array)\n",
        "\n",
        "        plt.figure(figsize=(12, 8))\n",
        "        plt.clf()\n",
        "\n",
        "        # رسم نقاط با اندازه متناسب با score\n",
        "        sizes = np.array(valid_scores) * 100  # تبدیل score به اندازه نقاط\n",
        "\n",
        "        scatter = plt.scatter(word_vectors_2d[:, 0], word_vectors_2d[:, 1],\n",
        "                            s=sizes, alpha=0.6, c=valid_scores, cmap='viridis')\n",
        "\n",
        "        # اضافه کردن برچسب‌ها\n",
        "        for i, word in enumerate(valid_words):\n",
        "            plt.annotate(f\"{word}\\n({valid_scores[i]:.3f})\",\n",
        "                        (word_vectors_2d[i, 0], word_vectors_2d[i, 1]),\n",
        "                        xytext=(5, 5), textcoords='offset points',\n",
        "                        fontsize=8, alpha=0.8)\n",
        "\n",
        "        plt.colorbar(scatter, label='Score')\n",
        "        plt.title('2D Visualization of Dictionary Words\\nColored by Score')\n",
        "        plt.xlabel('Dimension 1')\n",
        "        plt.ylabel('Dimension 2')\n",
        "\n",
        "        # ذخیره نمودار\n",
        "        plot_path = base_path + 'dictionary_embedding_plot.jpg'\n",
        "        plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
        "        plt.close()\n",
        "        verify_file_exists(plot_path, \"Plot\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"✗ Error creating visualization: {str(e)}\")\n",
        "        return\n",
        "\n",
        "    # 7. ایجاد و ذخیره فایل Word\n",
        "    print(\"\\n=== Step 7: Creating Word document ===\")\n",
        "    try:\n",
        "        doc = Document()\n",
        "\n",
        "        doc.add_heading('Word Embedding Analysis Report', 0)\n",
        "\n",
        "        from datetime import datetime\n",
        "        doc.add_paragraph(f'Generated on: {datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")}')\n",
        "\n",
        "        doc.add_heading('Analysis Parameters', level=1)\n",
        "        doc.add_paragraph(f'Score threshold: {threshold}')\n",
        "        doc.add_paragraph(f'Total words analyzed: {len(valid_words)}')\n",
        "\n",
        "        doc.add_heading('Word Scores', level=1)\n",
        "        table = doc.add_table(rows=1, cols=2)\n",
        "        table.style = 'Table Grid'\n",
        "        table.rows[0].cells[0].text = 'Word'\n",
        "        table.rows[0].cells[1].text = 'Score'\n",
        "\n",
        "        for word, score in zip(valid_words, valid_scores):\n",
        "            row = table.add_row()\n",
        "            row.cells[0].text = word\n",
        "            row.cells[1].text = f'{score:.3f}'\n",
        "\n",
        "        doc.add_heading('Visualization', level=1)\n",
        "        doc.add_paragraph(\"\"\"\n",
        "        The visualization shows the semantic relationships between words in a two-dimensional space.\n",
        "        - Larger circles indicate higher scores\n",
        "        - Colors indicate the score (darker = higher score)\n",
        "        - Words that appear closer together share similar semantic meanings\n",
        "        - Distance between words represents semantic similarity\n",
        "        \"\"\")\n",
        "\n",
        "        doc.add_picture(plot_path, width=docx.shared.Inches(6))\n",
        "\n",
        "        word_doc_path = base_path + 'word_embedding_results.docx'\n",
        "        doc.save(word_doc_path)\n",
        "        verify_file_exists(word_doc_path, \"Word document\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"✗ Error creating Word document: {str(e)}\")\n",
        "        return\n",
        "\n",
        "    print(\"\\n=== Analysis completed successfully! ===\")\n",
        "    print(f\"Files saved in: {base_path}\")\n",
        "    print(\"1. Plot: dictionary_embedding_plot.jpg\")\n",
        "    print(\"2. Report: word_embedding_results.docx\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ]
}