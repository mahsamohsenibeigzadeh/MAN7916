# Week 7

## Topic: (Large) Language Models

## Discussion Leader: Xin

### Readings - Required

- Visual Storytelling Team & Murgia, M. (2023). Generative AI exists because of the transformer. *Financial Times* [Link](https://ig.ft.com/generative-ai/)
- Jurafsky, D., & Martin, J. H. (2024). Speech and language processing (3rd Edition). Pearson.
  - Read: [Chapter 11: Masked Language Models](https://web.stanford.edu/~jurafsky/slp3/11.pdf) - Don't worry about the math, focus on the concepts. You may need to read tidbits from chapters [9](https://web.stanford.edu/~jurafsky/slp3/9.pdf)-[10](https://web.stanford.edu/~jurafsky/slp3/10.pdf) to understand the concepts... or provide the confusing bits to ChatGPT and have it explain it to you in simpler terms...üòâ. Oh the irony!
  - Read: [Chapter 12: Model Alignment, Prompting, and In-Context Learning](https://web.stanford.edu/~jurafsky/slp3/12.pdf).
- Fyffe, S., Lee, P., & Kaplan, S. (2024). ‚ÄúTransforming‚Äù personality scale development: Illustrating the potential of state-of-the-art natural language processing. *Organizational Research Methods, 27*(2), 265-300.
- Speer, A. B., Perrotta, J., & Kordsmeyer, T. L. (2023). Taking It Easy: Off-the-Shelf Versus Fine-Tuned Supervised Modeling of Performance Appraisal Text. *Organizational Research Methods*.



### Assignment

[Assignment 6](../assignments/materials/week_6/instructions.md)